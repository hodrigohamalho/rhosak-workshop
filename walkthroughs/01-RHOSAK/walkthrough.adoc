:walkthrough: Red Hat OpenShift Streams for Apache Kafka
:codeready-url: {che-url}
:openshift-url: {openshift-host}
:user-password: openshift

= Red Hat OpenShift Streams for Apache Kafka

Learn how to use asynchronous communication in a event driven architecture using Kafka as the event broker.

Know and explore the Managed Kafka Offering from Red Hat, Red Hat Openshift Streams for Apache Kafka (RHOSAK).

== Event Driven Architecture Introduction

Event Driven Architecture Introduction

In an Event Driven Architecture, an event notification is generated, the system captures what happened and waits to provide the response back.  The application that got the notification might either reply right away or wait till the status changes.

Event Driven Architecture allows for more flexible, scalable, contextual, and responsive digital business systems. This is why this architecture style has been gaining popularity.

The key rationale for leveraging Apache Kafka for an Event Driven system is the decoupling of microservices and the development of a Kafka pipeline to connect producers and consumers. Instead of checking for new data, you may just listen to a certain event and take action. Kafka provides a scalable hybrid approach that incorporates both Processing and Messaging.

Another advantage of using Event Driven Architecture with Kafka is that, unlike messaging-oriented systems, events published in Kafka are not removed as soon as they are consumed. They are removed once a specific amount of time has passed.

During their lifetime, they may be read by a variety of consumers, allowing them to respond to a variety of use cases.

[time=5]
== Access the Red Hat Developers portal

Access the portal and log-in. In case you do not have registered yet, please do your registration.

https://developers.redhat.com

image::./images/developer-portal.png[]

{empty} +

Do the Login

image::./images/developer-portal-login.png[]

{empty} +

In case that that you need to do your registration, you should see a form like that:

image::./images/developer-portal-registration.png[]

Access the Red Hat Streams for Apache Kafka Page, and click on `Create instance`.

https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview

image::./images/stream-for-apache-kafka-page.png[]

{empty} +

If it's your first access, you should see a screen like this:

image::./images/create-kafka-instance.png[]

{empty} +

Access the Learning resources section from the left side menu.

image::./images/learning-resources.png[]

{empty} +

Select the `Getting Started with Red Hat Openshift Streams for Apache Kafka` quickstart.

image::./images/learning-resources-2.png[]

{empty} +

[time=25]
== Getting Started with Red Hat Openshift Streams for Apache Kafka

Follow the lab instructions from the portal. You in the end of this lab you must conclude the following steps:

* Create a Kafka instance
* Copied the connection information
* Create a Service Account 
* Copied the ClientID and Client Secret from the Service Account
* Set the Access Control for the Kafka instance


[time=20]
== Getting started with Red Hat OpenShift Connectors

Access the Learning resources section from the left side menu.

Select the `Getting started with Red Hat OpenShift Connectors` quickstart.

image::./images/learning-resources-3.png[]

{empty} +

NOTE: The creation of a new topic is not necessary since we can use the topic `my-first-kafka-topic` that was created in the previous lab.

Click on the `Getting started with Red Hat OpenShift Connectors` to open the instructions.

image::./images/connectors/01-learning-resource.png[]

{empty} +

From the side menu, select `Connectors` > `Connectors Instance`. Click on Create a connection instance.

image::./images/connectors/02-connector-instances.png[]

{empty} +

In Connector type `data` in the search box, so select the `Data Generator source`.

image::./images/connectors/03-create-connector.png[]

{empty} +

Select the Kafka instance previously created.

image::./images/connectors/03-create-connector-2.png[]

{empty} +

Click on `Create a preview namespace`.

image::./images/connectors/03-create-connector-3.png[]

{empty} +

Click on `Create a preview namespace`.

image::./images/connectors/03-create-connector-4.png[]

{empty} +

Confirm it.

image::./images/connectors/03-create-connector-5.png[]

{empty} +

Select the preview namespace.

image::./images/connectors/03-create-connector-6.png[]

{empty} +

The topic `my-first-kafka-topic` was created in the previous lab.

Fill the form with the following instructions: 

. Topic name: `my-first-kafka-topic`
. Content Type: `text/plain`
. Message: `Hello World!`
. Period: 10000

image::./images/connectors/03-create-connector-7.png[]

{empty} +

Review the connector information.

image::./images/connectors/03-create-connector-8.png[]

{empty} +

Check if it's deployed correctly.

image::./images/connectors/03-create-connector-9.png[]

{empty} +

Access the `my-first-kafka-topic` Kafka topic in your Kafka instance.

image::./images/connectors/04-access-kafka-topic.png[]

{empty} +

Go to the `Messages` tab. See if the messages are being stored succesfully in Kafka.

image::./images/connectors/04-access-kafka-topic-messages.png[]

{empty} +

Fine, we succesfully created the Source Connector. 
A Kafka Producer, so right now let's create the Kafka Consumer (Sink Connector).

On the Create Connector page, type: `http sink`.

image::./images/connectors/05-create-sink.png[]

{empty} +

Select the Kafka instance previously created.

image::./images/connectors/05-create-sink-1.png[]

{empty} +

Select the namespace already created.

image::./images/connectors/05-create-sink-2.png[]

{empty} +

On the configuration fill with:

* Instance name: kafka-consumer
* Client ID: The client ID generated in the previous lab
* Client Secret: The client ID generated in the previous lab

image::./images/connectors/05-create-sink-3.png[]

{empty} +

Access the https://webhook.site to get your URL webhook.

image::./images/connectors/05-create-sink-4.png[]

{empty} +

Fill the form with: 

* Consumes Format: `application/octet-stream`
* Method: `POST`
* URL: URL FROM webhook.site
* Topic name: `my-first-kafka-topic`

image::./images/connectors/05-create-sink-5.png[]

{empty} +

In the Error Handling option select: `log`.

image::./images/connectors/05-create-sink-6.png[]

{empty} +

Check if everything is correctly deployed.

image::./images/connectors/05-create-sink-7.png[]

{empty} +

Check if the messages are being generated in the webbook tab from your browser.

image::./images/connectors/06-check-messages-webhook.png[]

{empty} +

Know that we are producing and consuming information from Kafka, let's take a look in the metrics from the Kafka instance dashboard.

image::./images/connectors/07-kafka-metrics.png[]

{empty} +

image::./images/connectors/07-kafka-metrics-1.png[]
